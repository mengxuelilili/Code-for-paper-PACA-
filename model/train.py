import torch
import torch.nn as nn
import numpy as np
import json
import math
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from scipy.stats import pearsonr
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split, KFold, ParameterGrid
from models.roformercnn import CombinedModel
import torch.optim as optim
import os, random
from datetime import datetime


# ---------------------------
# è®¾ç½®éšæœºç§å­
# ---------------------------
def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


set_seed(42)


# ---------------------------
# æ•°æ®é›†ç±»
# ---------------------------
class CustomDataset(Dataset):
    def __init__(self, X_a, X_b, antigen, y):
        self.X_a = torch.tensor(X_a, dtype=torch.float32)
        self.X_b = torch.tensor(X_b, dtype=torch.float32)
        self.antigen = torch.tensor(antigen, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return self.X_a[idx], self.X_b[idx], self.antigen[idx], self.y[idx]


# ---------------------------
# åŠ è½½æ‰€æœ‰æ•°æ®é›†
# ---------------------------
def load_all_datasets():
    dataset_paths = {
        'train_data': "/tmp/AbAgCDR/data/train_data.pt",
        'abbind_data': "/tmp/AbAgCDR/data/abbind_data.pt",
        'sabdab_data': "/tmp/AbAgCDR/data/sabdab_data.pt",
        'skempi_data': "/tmp/AbAgCDR/data/skempi_data.pt"
    }

    datasets = {}
    for name, path in dataset_paths.items():
        if not os.path.exists(path):
            print(f"âš ï¸ è­¦å‘Š: {path} ä¸å­˜åœ¨ï¼Œè·³è¿‡ {name}")
            continue

        data = torch.load(path)
        if "X_a_train" in data:
            X_a, X_b, antigen, y = data["X_a_train"], data["X_b_train"], data["antigen_train"], data["y_train"]
        else:
            X_a, X_b, antigen, y = data["X_a"], data["X_b"], data["antigen"], data["y"]

        datasets[name] = {
            "X_a": X_a.detach().cpu().numpy(),
            "X_b": X_b.detach().cpu().numpy(),
            "antigen": antigen.detach().cpu().numpy(),
            "y": y.detach().cpu().numpy()
        }

        print(f"âœ… å·²åŠ è½½ {name}ï¼Œæ ·æœ¬æ•°: {len(y)}")

    return datasets


# ---------------------------
# ç‹¬ç«‹æ•°æ®é›†åˆ’åˆ†
# ---------------------------
def split_dataset_independently(data, test_size=0.2, val_size=0.2):
    """ç‹¬ç«‹åˆ’åˆ†æ¯ä¸ªæ•°æ®é›†ï¼Œé¿å…æ•°æ®æ³„éœ²"""
    X_a, X_b, antigen, y = data['X_a'], data['X_b'], data['antigen'], data['y']

    # å…ˆåˆ’åˆ†è®­ç»ƒ+éªŒè¯ å’Œ æµ‹è¯•
    X_a_temp, X_a_test, X_b_temp, X_b_test, antigen_temp, antigen_test, y_temp, y_test = train_test_split(
        X_a, X_b, antigen, y, test_size=test_size, random_state=42
    )

    # å†ä»è®­ç»ƒ+éªŒè¯ä¸­åˆ’åˆ†å‡ºéªŒè¯é›†
    val_ratio = val_size / (1 - test_size)
    X_a_train, X_a_val, X_b_train, X_b_val, antigen_train, antigen_val, y_train, y_val = train_test_split(
        X_a_temp, X_b_temp, antigen_temp, y_temp, test_size=val_ratio, random_state=42
    )

    return {
        'train': {'X_a': X_a_train, 'X_b': X_b_train, 'antigen': antigen_train, 'y': y_train},
        'val': {'X_a': X_a_val, 'X_b': X_b_val, 'antigen': antigen_val, 'y': y_val},
        'test': {'X_a': X_a_test, 'X_b': X_b_test, 'antigen': antigen_test, 'y': y_test}
    }


# ---------------------------
# CDRåŒºåŸŸ
# ---------------------------
def getCDRPos(_loop, cdr_scheme='chothia'):
    CDRS = {
        'L1': ['24', '25', '26', '27', '28', '29', '30', '30A', '30B', '30C', '30D', '30E', '30F', '30G', '30H', '30I',
               '31', '32', '33', '34'],
        'L2': ['50', '51', '51A', '52', '52A', '52B', '52C', '52D', '53', '54', '55', '56'],
        'L3': ['89', '90', '91', '92', '93', '94', '95', '95A', '95B', '95C', '95D', '95E', '95F', '95G', '95H', '95I',
               '95J', '96', '97'],
        'H1': ['26', '27', '28', '29', '30', '31', '31A', '31B', '31C', '31D', '31E', '31F', '31G', '31H', '31I', '31J',
               '32'],
        'H2': ['52', '52A', '52B', '52C', '52D', '52E', '52F', '52G', '52H', '52I', '52J', '52K', '52L', '52M', '52N',
               '52O', '53', '54', '55', '56'],
        'H3': ['95', '96', '97', '98', '99', '100', '100A', '100B', '100C', '100D', '100E', '100F', '100G', '100H',
               '100I', '100J', '100K',
               '100L', '100M', '100N', '100O', '100P', '100Q', '100R', '100S', '100T', '100U', '100V', '100W', '100X',
               '100Y', '100Z', '101', '102']
    }
    return CDRS[_loop]


# ---------------------------
# åºåˆ—å¡«å…… (å¯¹é½åˆ°æœ€å¤§é•¿åº¦)
# ---------------------------
def pad_to_maxlen(arrays, max_len=None):
    """å°†å¤šä¸ª numpy æ•°ç»„ pad æˆ–æˆªæ–­åˆ°ç›¸åŒé•¿åº¦"""
    if max_len is None:
        max_len = max(arr.shape[1] for arr in arrays)
    padded = []
    for arr in arrays:
        if arr.shape[1] < max_len:
            pad_width = ((0, 0), (0, max_len - arr.shape[1]), (0, 0))
            arr_padded = np.pad(arr, pad_width, mode="constant")
            padded.append(arr_padded)
        elif arr.shape[1] > max_len:
            padded.append(arr[:, :max_len, :])
        else:
            padded.append(arr)
    return np.vstack(padded), max_len


# ---------------------------
# å¡«å……åºåˆ—
# ---------------------------
def pad_sequence(sequences, max_len):
    padded_seqs = []
    for seq in sequences:
        if seq.dim() == 2:
            seq = seq.unsqueeze(0)
        seq_len = seq.shape[1]
        if seq_len < max_len:
            pad_len = max_len - seq_len
            padding = torch.zeros((seq.shape[0], pad_len, seq.shape[2]), device=seq.device)
            padded_seqs.append(torch.cat([seq, padding], dim=1))
        elif seq_len > max_len:
            padded_seqs.append(seq[:, :max_len, :])
        else:
            padded_seqs.append(seq)
    return torch.stack(padded_seqs)


# ---------------------------
# éªŒè¯å‡½æ•°
# ---------------------------
def validate_model(model, test_loader, device='cuda'):
    model.eval()
    all_labels, all_preds = [], []

    with torch.no_grad():
        for X_a, X_b, antigen, y in test_loader:
            X_a, X_b, antigen = X_a.to(device), X_b.to(device), antigen.to(device)
            max_len = max(X_a.shape[1], X_b.shape[1], antigen.shape[1])
            light_chain = pad_sequence(X_a, max_len)
            heavy_chain = pad_sequence(X_b, max_len)
            antigen = pad_sequence(antigen, max_len)

            preds = model(heavy_chain, light_chain, antigen).view(-1, 1)
            y = y.view(-1, 1)
            all_labels.extend(y.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())

    all_labels = np.array(all_labels).flatten()
    all_preds = np.array(all_preds).flatten()

    mse = mean_squared_error(all_labels, all_preds)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(all_labels, all_preds)
    r2 = r2_score(all_labels, all_preds)
    pcc = np.corrcoef(all_labels, all_preds)[0, 1] if len(set(all_labels)) > 1 else 0

    return mse, rmse, mae, r2, pcc, all_labels, all_preds


# ---------------------------
# æ¨¡å‹è®­ç»ƒå™¨
# ---------------------------
class StableModelTrainer:
    def __init__(self, model, loaders, val_loader, params, device, dataset_weights):
        self.model = model
        self.loaders = loaders
        self.val_loader = val_loader
        self.params = params
        self.device = device
        self.dataset_weights = dataset_weights
        self.loader_iters = {name: iter(ld) for name, ld in loaders.items()}

    def train(self):
        model = self.model.to(self.device)
        criterion = nn.MSELoss()

        optimizer = optim.Adam(model.parameters(),
                               lr=self.params['lr'],
                               weight_decay=self.params.get('weight_decay', 1e-4))

        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, 'min',
            patience=self.params.get('scheduler_patience', 5),
            factor=self.params.get('lr_factor', 0.5),
            min_lr=self.params.get('min_lr', 1e-6),
            verbose=False
        )

        dataset_names = list(self.loaders.keys())
        dataset_probs = [self.dataset_weights.get(name, 1.0) for name in dataset_names]
        total_weight = sum(dataset_probs)
        dataset_probs = [p / total_weight for p in dataset_probs]

        best_val_mse = np.inf
        best_val_r2 = -np.inf
        best_model_wts = None
        counter = 0
        epochs = self.params['epochs']
        steps_per_epoch = self.params.get('steps_per_epoch', 200)

        for epoch in range(1, epochs + 1):
            model.train()
            total_loss = 0.0

            for step in range(steps_per_epoch):
                dataset_name = random.choices(dataset_names, weights=dataset_probs, k=1)[0]
                loader = self.loaders[dataset_name]

                try:
                    batch = next(self.loader_iters[dataset_name])
                except StopIteration:
                    self.loader_iters[dataset_name] = iter(loader)
                    batch = next(self.loader_iters[dataset_name])

                X_a, X_b, antigen, y = [x.to(self.device) for x in batch]

                max_len = max(X_a.shape[1], X_b.shape[1], antigen.shape[1])
                light_chain = pad_sequence(X_a, max_len)
                heavy_chain = pad_sequence(X_b, max_len)
                antigen = pad_sequence(antigen, max_len)

                optimizer.zero_grad()
                preds = model(heavy_chain, light_chain, antigen).view(-1, 1)
                y = y.view(-1, 1)
                loss = criterion(preds, y)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
                optimizer.step()
                total_loss += loss.item()

            avg_train_loss = total_loss / steps_per_epoch

            # éªŒè¯
            val_mse, val_rmse, val_mae, val_r2, val_pcc, _, _ = validate_model(
                model, self.val_loader, device=self.device
            )

            # æ¯ä¸ª epoch æ‰“å°
            print(f"Epoch [{epoch}/{epochs}] "
                  f"Train Loss: {avg_train_loss:.4f} | "
                  f"Val MSE: {val_mse:.4f} | Val RMSE: {val_rmse:.4f} | Val MAE: {val_mae:.4f} | "
                  f"RÂ²: {val_r2:.4f} | PCC: {val_pcc:.4f}")

            scheduler.step(val_mse)

            # åˆ¤æ–­æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹ï¼Œåªæ‰“å°ä¸ä¿å­˜
            improvement = False
            if val_mse < best_val_mse and val_r2 > best_val_r2:
                improvement = True
            elif val_mse < best_val_mse * 1.02 and val_r2 > best_val_r2 + 0.01:
                improvement = True
            elif val_mse < best_val_mse * 0.98:
                improvement = True

            if improvement:
                best_val_mse = val_mse
                best_val_r2 = val_r2
                best_model_wts = model.state_dict().copy()
                counter = 0
                print(f"ğŸ† æ–°æœ€ä½³æ¨¡å‹åœ¨ Epoch {epoch} è¢«å‘ç°! Val MSE: {val_mse:.4f}, RÂ²: {val_r2:.4f}")
            else:
                counter += 1
                if counter >= self.params['patience']:
                    print(f"âš ï¸ æ—©åœäºç¬¬ {epoch} è½®")
                    break

        # è®­ç»ƒç»“æŸååŠ è½½æœ€ä½³æ¨¡å‹æƒé‡
        if best_model_wts:
            model.load_state_dict(best_model_wts)
        return model, best_val_mse


# ---------------------------
# ç½‘æ ¼æœç´¢
# ---------------------------
def perform_grid_search(datasets, params_grid, dataset_weights):
    """ç½‘æ ¼æœç´¢æœ€ä½³å‚æ•°"""
    print("å¼€å§‹ç½‘æ ¼æœç´¢æœ€ä½³å‚æ•°...")

    # å…ˆpadå¯¹é½æ‰€æœ‰æ•°æ®
    all_X_a, max_len_a = pad_to_maxlen([d['X_a'] for d in datasets.values()])
    all_X_b, max_len_b = pad_to_maxlen([d['X_b'] for d in datasets.values()])
    all_antigen, max_len_ag = pad_to_maxlen([d['antigen'] for d in datasets.values()])
    all_y = np.concatenate([d['y'] for d in datasets.values()])

    print(f"ç»Ÿä¸€å max_len: X_a={max_len_a}, X_b={max_len_b}, antigen={max_len_ag}")

    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†ï¼ˆç”¨äºå‚æ•°é€‰æ‹©ï¼‰
    X_a_train, X_a_val, X_b_train, X_b_val, antigen_train, antigen_val, y_train, y_val = train_test_split(
        all_X_a, all_X_b, all_antigen, all_y, test_size=0.2, random_state=42
    )

    val_data = {'X_a': X_a_val, 'X_b': X_b_val, 'antigen': antigen_val, 'y': y_val}

    best_score = np.inf
    best_r2 = -np.inf
    best_params = None
    best_model = None

    val_dataset = CustomDataset(val_data['X_a'], val_data['X_b'], val_data['antigen'], val_data['y'])
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

    param_combinations = list(ParameterGrid(params_grid))

    for i, params in enumerate(param_combinations):
        print(f"\n{'=' * 50}")
        print(f"å°è¯•å‚æ•°ç»„åˆ {i + 1}/{len(param_combinations)}: {params}")
        print(f"{'=' * 50}")

        # ä½¿ç”¨å¯¹é½åçš„æ•°æ®é‡æ„datasetsï¼ˆä»…ç”¨äºå‚æ•°æœç´¢ï¼‰
        start_idx = 0
        search_datasets = {}
        for name, original_data in datasets.items():
            end_idx = start_idx + len(original_data['y'])
            search_datasets[name] = {
                'X_a': all_X_a[start_idx:end_idx],
                'X_b': all_X_b[start_idx:end_idx],
                'antigen': all_antigen[start_idx:end_idx],
                'y': all_y[start_idx:end_idx]
            }
            start_idx = end_idx

        loaders = {}
        for name, d in search_datasets.items():
            train_dataset = CustomDataset(d['X_a'], d['X_b'], d['antigen'], d['y'])
            loaders[name] = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True, drop_last=True)

        cdr_boundaries_heavy = [getCDRPos('H1'), getCDRPos('H2'), getCDRPos('H3')]
        cdr_boundaries_light = [getCDRPos('L1'), getCDRPos('L2'), getCDRPos('L3')]

        model = CombinedModel(
            cdr_boundaries_heavy, cdr_boundaries_light,
            num_heads=2, embed_dim=532, antigen_embed_dim=500
        )

        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        trainer = StableModelTrainer(model, loaders, val_loader, params, device, dataset_weights)

        trained_model, val_score = trainer.train()

        final_mse, _, _, final_r2, final_pcc, _, _ = validate_model(trained_model, val_loader, device)

        print(f"\nå‚æ•°ç»„åˆ {i + 1} æœ€ç»ˆç»“æœ:")
        print(f"MSE: {final_mse:.4f} | RÂ²: {final_r2:.4f} | PCC: {final_pcc:.4f}")

        combined_score = final_mse - final_r2

        if combined_score < best_score or (abs(combined_score - best_score) < 0.1 and final_r2 > best_r2):
            best_score = combined_score
            best_r2 = final_r2
            best_params = params
            best_model = trained_model
            print(f"ğŸ† æ‰¾åˆ°æ›´å¥½çš„æ¨¡å‹! MSE: {final_mse:.4f}, RÂ²: {final_r2:.4f}")

    return best_model, best_params, best_score


# ---------------------------
# KæŠ˜äº¤å‰éªŒè¯
# ---------------------------
def cross_validation(datasets, best_params, k=3):
    """å¯¹æ¯ä¸ªæ•°æ®é›†è¿›è¡ŒKæŠ˜äº¤å‰éªŒè¯"""
    results = {}

    for dataset_name, data in datasets.items():
        print(f"\nğŸ“Š {dataset_name} - {k}æŠ˜äº¤å‰éªŒè¯")

        # ç‹¬ç«‹åˆ’åˆ†å½“å‰æ•°æ®é›†
        split_data = split_dataset_independently(data)

        # åˆå¹¶è®­ç»ƒå’ŒéªŒè¯æ•°æ®ç”¨äºäº¤å‰éªŒè¯
        train_val_data = {
            'X_a': np.vstack([split_data['train']['X_a'], split_data['val']['X_a']]),
            'X_b': np.vstack([split_data['train']['X_b'], split_data['val']['X_b']]),
            'antigen': np.vstack([split_data['train']['antigen'], split_data['val']['antigen']]),
            'y': np.concatenate([split_data['train']['y'], split_data['val']['y']])
        }

        X_a, X_b, antigen, y = train_val_data['X_a'], train_val_data['X_b'], train_val_data['antigen'], train_val_data['y']

        kfold = KFold(n_splits=k, shuffle=True, random_state=42)
        fold_results = []

        for fold, (train_idx, val_idx) in enumerate(kfold.split(X_a)):
            print(f"  æŠ˜ {fold + 1}/{k}")

            # åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯é›†
            train_dataset = CustomDataset(
                X_a[train_idx], X_b[train_idx], antigen[train_idx], y[train_idx]
            )
            val_dataset = CustomDataset(
                X_a[val_idx], X_b[val_idx], antigen[val_idx], y[val_idx]
            )

            train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True, drop_last=True)
            val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

            # åˆ›å»ºæ¨¡å‹
            cdr_boundaries_heavy = [getCDRPos('H1'), getCDRPos('H2'), getCDRPos('H3')]
            cdr_boundaries_light = [getCDRPos('L1'), getCDRPos('L2'), getCDRPos('L3')]
            model = CombinedModel(
                cdr_boundaries_heavy, cdr_boundaries_light,
                num_heads=2, embed_dim=532, antigen_embed_dim=500
            )

            # è®­ç»ƒæ¨¡å‹
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            train_loaders = {dataset_name: train_loader}
            trainer = StableModelTrainer(model, train_loaders, val_loader, best_params, device, {dataset_name: 1.0})

            trained_model, _ = trainer.train()

            # è¯„ä¼°
            mse, rmse, mae, r2, pcc, _, _ = validate_model(trained_model, val_loader, device)
            metrics = {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'R2': r2, 'PCC': pcc}
            fold_results.append(metrics)

            print(f"    RÂ²: {r2:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, PCC: {pcc:.4f}")

        # è®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®
        avg_metrics = {}
        std_metrics = {}
        for metric in fold_results[0].keys():
            values = [r[metric] for r in fold_results]
            avg_metrics[metric] = np.mean(values)
            std_metrics[metric] = np.std(values)

        results[dataset_name] = {
            'mean': avg_metrics,
            'std': std_metrics
        }

        print(f"  ğŸ“Š {dataset_name} äº¤å‰éªŒè¯æ€»ç»“:")
        print(f"    MSE: {avg_metrics['MSE']:.4f} Â± {std_metrics['MSE']:.4f}")
        print(f"    RMSE: {avg_metrics['RMSE']:.4f} Â± {std_metrics['RMSE']:.4f}")
        print(f"    MAE: {avg_metrics['MAE']:.4f} Â± {std_metrics['MAE']:.4f}")
        print(f"    RÂ²: {avg_metrics['R2']:.4f} Â± {std_metrics['R2']:.4f}")
        print(f"    PCC: {avg_metrics['PCC']:.4f} Â± {std_metrics['PCC']:.4f}")

    return results


# ---------------------------
# æœ€ç»ˆæµ‹è¯•è¯„ä¼°
# ---------------------------
def final_test_evaluation(datasets, best_params, device):
    """åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šè¯„ä¼°"""
    print(f"\nğŸ”¬ æœ€ç»ˆæµ‹è¯•è¯„ä¼°")

    # æ”¶é›†æ‰€æœ‰è®­ç»ƒæ•°æ®
    all_train_data = {'X_a': [], 'X_b': [], 'antigen': [], 'y': []}
    all_test_data = {}

    for dataset_name, data in datasets.items():
        split_data = split_dataset_independently(data)

        # æ”¶é›†è®­ç»ƒæ•°æ®ï¼ˆè®­ç»ƒé›†+éªŒè¯é›†ï¼‰
        all_train_data['X_a'].append(split_data['train']['X_a'])
        all_train_data['X_a'].append(split_data['val']['X_a'])
        all_train_data['X_b'].append(split_data['train']['X_b'])
        all_train_data['X_b'].append(split_data['val']['X_b'])
        all_train_data['antigen'].append(split_data['train']['antigen'])
        all_train_data['antigen'].append(split_data['val']['antigen'])
        all_train_data['y'].append(split_data['train']['y'])
        all_train_data['y'].append(split_data['val']['y'])

        # ä¿å­˜æµ‹è¯•æ•°æ®
        all_test_data[dataset_name] = split_data['test']

    # å…ˆå¯¹é½æ‰€æœ‰æ•°æ®å†åˆå¹¶
    print("å¯¹é½è®­ç»ƒæ•°æ®ç»´åº¦...")
    aligned_X_a, max_len_a = pad_to_maxlen(all_train_data['X_a'])
    aligned_X_b, max_len_b = pad_to_maxlen(all_train_data['X_b'])
    aligned_antigen, max_len_ag = pad_to_maxlen(all_train_data['antigen'])
    aligned_y = np.concatenate(all_train_data['y'])

    print(f"å¯¹é½åç»´åº¦: X_a={max_len_a}, X_b={max_len_b}, antigen={max_len_ag}")

    # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    train_dataset = CustomDataset(aligned_X_a, aligned_X_b, aligned_antigen, aligned_y)
    train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True, drop_last=True)

    cdr_boundaries_heavy = [getCDRPos('H1'), getCDRPos('H2'), getCDRPos('H3')]
    cdr_boundaries_light = [getCDRPos('L1'), getCDRPos('L2'), getCDRPos('L3')]
    final_model = CombinedModel(
        cdr_boundaries_heavy, cdr_boundaries_light,
        num_heads=2, embed_dim=532, antigen_embed_dim=500
    )

    train_loaders = {'combined': train_loader}
    trainer = StableModelTrainer(final_model, train_loaders, train_loader, best_params, device, {'combined': 1.0})

    print("è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
    final_trained_model, _ = trainer.train()

    # åœ¨æ¯ä¸ªç‹¬ç«‹æµ‹è¯•é›†ä¸Šè¯„ä¼°
    test_results = {}
    for dataset_name, test_data in all_test_data.items():
        print(f"è¯„ä¼° {dataset_name} æµ‹è¯•é›†...")

        test_dataset = CustomDataset(test_data['X_a'], test_data['X_b'],
                                     test_data['antigen'], test_data['y'])
        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        mse, rmse, mae, r2, pcc, y_true, y_pred = validate_model(final_trained_model, test_loader, device)
        metrics = {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'R2': r2, 'PCC': pcc}

        test_results[dataset_name] = {'metrics': metrics}

        print(f"  ğŸ“Š {dataset_name} æµ‹è¯•ç»“æœ:")
        print(f"    MSE: {mse:.4f}")
        print(f"    RMSE: {rmse:.4f}")
        print(f"    MAE: {mae:.4f}")
        print(f"    RÂ²: {r2:.4f}")
        print(f"    PCC: {pcc:.4f}")

    return final_trained_model, test_results


# ---------------------------
# ä¿å­˜ç»“æœï¼ˆç®€åŒ–ç‰ˆï¼‰
# ---------------------------
def save_results(cv_results, test_results, best_model, best_params, best_score, save_dir="/tmp/AbAgCDR"):
    """ä¿å­˜ç»“æœï¼Œåªä¿å­˜æœ€ç»ˆæœ€ä½³æ¨¡å‹"""

    # åˆ›å»ºæ¨¡å‹å’Œç»“æœç›®å½•
    model_dir = os.path.join(save_dir, "model")
    results_dir = os.path.join(save_dir, "results")
    os.makedirs(model_dir, exist_ok=True)
    os.makedirs(results_dir, exist_ok=True)

    # 1. ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆå­¦ä¹ åŸä»£ç æ–¹å¼ï¼‰
    model_save_path = os.path.join(model_dir, 'stable_bestmodel_weighted3.pth')
    torch.save({
        'model_state_dict': best_model.state_dict(),
        'params': best_params,
        'val_score': best_score,
        'timestamp': datetime.now().isoformat()
    }, model_save_path)
    print(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {model_save_path}")

    # 2. ä¿å­˜äº¤å‰éªŒè¯ç»“æœ
    cv_file = os.path.join(results_dir, 'cross_validation_results.json')
    cv_clean = {}
    for dataset, results in cv_results.items():
        cv_clean[dataset] = {
            'mean': {k: float(v) for k, v in results['mean'].items()},
            'std': {k: float(v) for k, v in results['std'].items()}
        }

    with open(cv_file, 'w') as f:
        json.dump(cv_clean, f, indent=2)

    # 3. ä¿å­˜æµ‹è¯•ç»“æœ
    test_file = os.path.join(results_dir, 'independent_test_results.json')
    test_clean = {}
    for dataset, results in test_results.items():
        test_clean[dataset] = {k: float(v) for k, v in results['metrics'].items()}

    with open(test_file, 'w') as f:
        json.dump(test_clean, f, indent=2)

    # 4. ä¿å­˜æœ€ä½³å‚æ•°
    params_file = os.path.join(results_dir, 'best_parameters.json')
    with open(params_file, 'w') as f:
        json.dump(best_params, f, indent=2)

    print(f"æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {save_dir}")
    print(f"   æ¨¡å‹: {model_dir}")
    print(f"   ç»“æœ: {results_dir}")


# ---------------------------
# ä¸»å‡½æ•°
# ---------------------------
def main():
    # å‚æ•°ç½‘æ ¼æœç´¢é…ç½®
    params_grid = {
        'lr': [0.0001, 0.0005],
        'patience': [15],
        'batch_size': [16, 32],
        'epochs': [50],
        'weight_decay': [1e-5],
        'scheduler_patience': [3],
        'lr_factor': [0.5],
        'min_lr': [1e-6],
        'steps_per_epoch': [200]
    }

    # æ•°æ®é›†æƒé‡é…ç½®
    dataset_weights = {
        'train_data': 0.5,
        'abbind_data': 0.2,
        'sabdab_data': 0.2,
        'skempi_data': 0.1
    }

    print("å¼€å§‹å®Œæ•´è®­ç»ƒå’Œè¯„ä¼°æµç¨‹")
    print("=" * 60)

    # 1. åŠ è½½æ•°æ®
    print("\næ­¥éª¤ 1: åŠ è½½æ‰€æœ‰æ•°æ®é›†...")
    datasets = load_all_datasets()

    if not datasets:
        print("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ•°æ®é›†!")
        return

    # 2. ç½‘æ ¼æœç´¢æœ€ä½³å‚æ•°
    print(f"\næ­¥éª¤ 2: ç½‘æ ¼æœç´¢æœ€ä½³å‚æ•°...")
    print(f"å‚æ•°ç»„åˆæ€»æ•°: {len(list(ParameterGrid(params_grid)))}")

    best_model, best_params, best_score = perform_grid_search(
        datasets, params_grid, dataset_weights
    )

    print(f"\næœ€ä½³å‚æ•°æ‰¾åˆ°:")
    for key, value in best_params.items():
        print(f"  {key}: {value}")
    print(f"æœ€ä½³éªŒè¯åˆ†æ•°: {best_score:.4f}")

    # 3. KæŠ˜äº¤å‰éªŒè¯
    print(f"\næ­¥éª¤ 3: ä½¿ç”¨æœ€ä½³å‚æ•°è¿›è¡ŒKæŠ˜äº¤å‰éªŒè¯...")
    cv_results = cross_validation(datasets, best_params, k=3)

    # 4. ç‹¬ç«‹æµ‹è¯•é›†è¯„ä¼°
    print(f"\næ­¥éª¤ 4: ç‹¬ç«‹æµ‹è¯•é›†è¯„ä¼°...")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    final_model, test_results = final_test_evaluation(datasets, best_params, device)

    # 5. ä¿å­˜ç»“æœ
    print(f"\næ­¥éª¤ 5: ä¿å­˜æ‰€æœ‰ç»“æœ...")
    save_results(cv_results, test_results, final_model, best_params, best_score)

    # 6. ç»“æœæ€»ç»“
    print("\n" + "=" * 60)
    print("æœ€ç»ˆè¯„ä¼°æ€»ç»“")
    print("=" * 60)

    print(f"\nç½‘æ ¼æœç´¢ç»“æœ:")
    print(f"  æœ€ä½³å‚æ•°: {best_params}")
    print(f"  æœ€ä½³åˆ†æ•°: {best_score:.4f}")

    print(f"\näº¤å‰éªŒè¯ç»“æœ:")
    for dataset, results in cv_results.items():
        print(f"  {dataset}:")
        for metric_name, mean_val in results['mean'].items():
            std_val = results['std'][metric_name]
            print(f"    {metric_name}: {mean_val:.4f} Â± {std_val:.4f}")

    print(f"\nç‹¬ç«‹æµ‹è¯•ç»“æœ:")
    for dataset, results in test_results.items():
        metrics = results['metrics']
        print(f"  {dataset}:")
        for metric_name, value in metrics.items():
            print(f"    {metric_name}: {value:.4f}")

    print(f"\nè®­ç»ƒå’Œè¯„ä¼°å®Œæˆ!")
    print(f"æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° /tmp/AbAgCDR/")

    # 7. æ•°æ®å®Œæ•´æ€§æ£€æŸ¥æ€»ç»“
    print(f"\næ•°æ®å®Œæ•´æ€§ä¿è¯:")
    print(f"  æ¯ä¸ªæ•°æ®é›†ç‹¬ç«‹åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†")
    print(f"  ç½‘æ ¼æœç´¢ä½¿ç”¨ç‹¬ç«‹éªŒè¯é›†é€‰æ‹©å‚æ•°")
    print(f"  äº¤å‰éªŒè¯åœ¨æ¯ä¸ªæ•°æ®é›†å†…éƒ¨ç‹¬ç«‹è¿›è¡Œ")
    print(f"  æœ€ç»ˆæµ‹è¯•ä½¿ç”¨å®Œå…¨ç‹¬ç«‹çš„æµ‹è¯•é›†")
    print(f"  æ— æ•°æ®æ³„éœ²é£é™©")


if __name__ == "__main__":
    main()
